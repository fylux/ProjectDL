{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLDIY_torch_pynb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJQRJAdvjLBi",
        "colab_type": "code",
        "outputId": "3cd0e7f8-e271-49ab-d12d-ff5937d2768a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import torchvision.datasets as dsets\n",
        "import torch.nn.functional as f\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "%matplotlib inline\n",
        "\n",
        "INPUT_SIZE = 24\n",
        "\n",
        "def weibull_loglik_discrete(y_true, ab_pred, name=None):\n",
        "    y_ = y_true[:, 0]\n",
        "    u_ = y_true[:, 1]\n",
        "    a_ = ab_pred[:, 0]\n",
        "    b_ = ab_pred[:, 1]\n",
        "\n",
        "    hazard0 = torch.pow((y_ + 1e-35) / a_, b_)\n",
        "    hazard1 = torch.pow((y_ + 1) / a_, b_)\n",
        "\n",
        "    return -1 * torch.mean(u_ * torch.log(torch.exp(hazard1 - hazard0) - 1.0) - hazard1)\n",
        "\n",
        "\n",
        "def weibull_loglik_continuous(y_true, ab_pred, name=None):\n",
        "    y_ = y_true[:, 0]\n",
        "    u_ = y_true[:, 1]\n",
        "    a_ = ab_pred[:, 0]\n",
        "    b_ = ab_pred[:, 1]\n",
        "\n",
        "    ya = (y_ + 1e-35) / a_\n",
        "    return -1 * torch.mean(u_ * (torch.log(b_) + b_ * torch.log(ya)) - torch.pow(ya, b_))\n",
        "\n",
        "\n",
        "def activate(ab):\n",
        "    a = torch.exp(ab[:, 0])\n",
        "    b = f.softplus(ab[:, 1])\n",
        "\n",
        "    a = torch.reshape(a, (a.size()[0], 1))\n",
        "    b = torch.reshape(b, (b.size()[0], 1))\n",
        "    return torch.cat((a, b), axis=1)\n",
        "\n",
        "\n",
        "def load_file(name):\n",
        "    with open(name, 'r') as file:\n",
        "        return np.loadtxt(file, delimiter=',')\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
        "            input_size=INPUT_SIZE,\n",
        "            hidden_size=20,         # rnn hidden unit\n",
        "            num_layers=2,           # number of rnn layer\n",
        "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
        "        )\n",
        "        self.out = nn.Linear(20, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape (batch, time_step, input_size)\n",
        "        # r_out shape (batch, time_step, output_size)\n",
        "        # h_n shape (n_layers, batch, hidden_size)\n",
        "        # h_c shape (n_layers, batch, hidden_size)\n",
        "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
        "\n",
        "        # choose r_out at the last time step\n",
        "        out = self.out(r_out[:, -1, :])\n",
        "        return activate(out)\n",
        "\n",
        "rnn = RNN()\n",
        "\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)  \n",
        "loss_func = weibull_loglik_discrete                       \n",
        "\n",
        "np.set_printoptions(suppress=True, threshold=10000)\n",
        "\n",
        "train = load_file('train.csv')\n",
        "test_x = load_file('test_x.csv')\n",
        "test_y = load_file('test_y.csv')\n",
        "\n",
        "\n",
        "all_x = np.concatenate((train[:, 2:26], test_x[:, 2:26]))\n",
        "all_x = normalize(all_x, axis=0)\n",
        "\n",
        "train[:, 2:26] = all_x[0:train.shape[0], :]\n",
        "test_x[:, 2:26] = all_x[train.shape[0]:, :]\n",
        "\n",
        "train[:, 0:2] -= 1\n",
        "test_x[:, 0:2] -= 1\n",
        "\n",
        "max_time = 100\n",
        "\n",
        "def build_data(engine, time, x, max_time, is_test):\n",
        "    out_y = np.empty((0, 2), dtype=np.float32)\n",
        "    out_x = np.empty((0, max_time, 24), dtype=np.float32)\n",
        "\n",
        "    for i in range(100):\n",
        "        print(\"Loading = \" + str(i))\n",
        "        max_engine_time = int(np.max(time[engine == i])) + 1\n",
        "\n",
        "        if is_test:\n",
        "            start = max_engine_time - 1\n",
        "        else:\n",
        "            start = 0\n",
        "\n",
        "        this_x = np.empty((0, max_time, 24), dtype=np.float32)\n",
        "\n",
        "        for j in range(start, max_engine_time):\n",
        "            engine_x = x[engine == i]\n",
        "\n",
        "            out_y = np.append(out_y, np.array((max_engine_time - j, 1), ndmin=2), axis=0)\n",
        "\n",
        "            xtemp = np.zeros((1, max_time, 24))\n",
        "            xtemp[:, max_time-min(j, 99)-1:max_time, :] = engine_x[max(0, j-max_time+1):j+1, :]\n",
        "            this_x = np.concatenate((this_x, xtemp))\n",
        "\n",
        "        out_x = np.concatenate((out_x, this_x))\n",
        "\n",
        "    return out_x, out_y\n",
        "\n",
        "train_x, train_y = build_data(train[:, 0], train[:, 1], train[:, 2:26], max_time, False)\n",
        "test_x = build_data(test_x[:, 0], test_x[:, 1], test_x[:, 2:26], max_time, True)[0]\n",
        "\n",
        "train_u = np.zeros((100, 1), dtype=np.float32)\n",
        "train_u += 1\n",
        "test_y = np.append(np.reshape(test_y, (100, 1)), train_u, axis=1)\n",
        "\n",
        "EPOCH = 200\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    b_x = torch.Tensor(train_x)              # reshape x to (batch, time_step, input_size)\n",
        "    b_y = torch.Tensor(train_y)                               # batch y\n",
        "\n",
        "    output = rnn(b_x)                               # rnn output\n",
        "    loss = loss_func(output, b_y)                   # loss\n",
        "    optimizer.zero_grad()                           # clear gradients for this training step\n",
        "    loss.backward()                                 # backpropagation, compute gradients\n",
        "    optimizer.step()                                # apply gradients\n",
        "    print(\"Epoch = {}\".format(epoch+1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading = 0\n",
            "Loading = 1\n",
            "Loading = 2\n",
            "Loading = 3\n",
            "Loading = 4\n",
            "Loading = 5\n",
            "Loading = 6\n",
            "Loading = 7\n",
            "Loading = 8\n",
            "Loading = 9\n",
            "Loading = 10\n",
            "Loading = 11\n",
            "Loading = 12\n",
            "Loading = 13\n",
            "Loading = 14\n",
            "Loading = 15\n",
            "Loading = 16\n",
            "Loading = 17\n",
            "Loading = 18\n",
            "Loading = 19\n",
            "Loading = 20\n",
            "Loading = 21\n",
            "Loading = 22\n",
            "Loading = 23\n",
            "Loading = 24\n",
            "Loading = 25\n",
            "Loading = 26\n",
            "Loading = 27\n",
            "Loading = 28\n",
            "Loading = 29\n",
            "Loading = 30\n",
            "Loading = 31\n",
            "Loading = 32\n",
            "Loading = 33\n",
            "Loading = 34\n",
            "Loading = 35\n",
            "Loading = 36\n",
            "Loading = 37\n",
            "Loading = 38\n",
            "Loading = 39\n",
            "Loading = 40\n",
            "Loading = 41\n",
            "Loading = 42\n",
            "Loading = 43\n",
            "Loading = 44\n",
            "Loading = 45\n",
            "Loading = 46\n",
            "Loading = 47\n",
            "Loading = 48\n",
            "Loading = 49\n",
            "Loading = 50\n",
            "Loading = 51\n",
            "Loading = 52\n",
            "Loading = 53\n",
            "Loading = 54\n",
            "Loading = 55\n",
            "Loading = 56\n",
            "Loading = 57\n",
            "Loading = 58\n",
            "Loading = 59\n",
            "Loading = 60\n",
            "Loading = 61\n",
            "Loading = 62\n",
            "Loading = 63\n",
            "Loading = 64\n",
            "Loading = 65\n",
            "Loading = 66\n",
            "Loading = 67\n",
            "Loading = 68\n",
            "Loading = 69\n",
            "Loading = 70\n",
            "Loading = 71\n",
            "Loading = 72\n",
            "Loading = 73\n",
            "Loading = 74\n",
            "Loading = 75\n",
            "Loading = 76\n",
            "Loading = 77\n",
            "Loading = 78\n",
            "Loading = 79\n",
            "Loading = 80\n",
            "Loading = 81\n",
            "Loading = 82\n",
            "Loading = 83\n",
            "Loading = 84\n",
            "Loading = 85\n",
            "Loading = 86\n",
            "Loading = 87\n",
            "Loading = 88\n",
            "Loading = 89\n",
            "Loading = 90\n",
            "Loading = 91\n",
            "Loading = 92\n",
            "Loading = 93\n",
            "Loading = 94\n",
            "Loading = 95\n",
            "Loading = 96\n",
            "Loading = 97\n",
            "Loading = 98\n",
            "Loading = 99\n",
            "Loading = 0\n",
            "Loading = 1\n",
            "Loading = 2\n",
            "Loading = 3\n",
            "Loading = 4\n",
            "Loading = 5\n",
            "Loading = 6\n",
            "Loading = 7\n",
            "Loading = 8\n",
            "Loading = 9\n",
            "Loading = 10\n",
            "Loading = 11\n",
            "Loading = 12\n",
            "Loading = 13\n",
            "Loading = 14\n",
            "Loading = 15\n",
            "Loading = 16\n",
            "Loading = 17\n",
            "Loading = 18\n",
            "Loading = 19\n",
            "Loading = 20\n",
            "Loading = 21\n",
            "Loading = 22\n",
            "Loading = 23\n",
            "Loading = 24\n",
            "Loading = 25\n",
            "Loading = 26\n",
            "Loading = 27\n",
            "Loading = 28\n",
            "Loading = 29\n",
            "Loading = 30\n",
            "Loading = 31\n",
            "Loading = 32\n",
            "Loading = 33\n",
            "Loading = 34\n",
            "Loading = 35\n",
            "Loading = 36\n",
            "Loading = 37\n",
            "Loading = 38\n",
            "Loading = 39\n",
            "Loading = 40\n",
            "Loading = 41\n",
            "Loading = 42\n",
            "Loading = 43\n",
            "Loading = 44\n",
            "Loading = 45\n",
            "Loading = 46\n",
            "Loading = 47\n",
            "Loading = 48\n",
            "Loading = 49\n",
            "Loading = 50\n",
            "Loading = 51\n",
            "Loading = 52\n",
            "Loading = 53\n",
            "Loading = 54\n",
            "Loading = 55\n",
            "Loading = 56\n",
            "Loading = 57\n",
            "Loading = 58\n",
            "Loading = 59\n",
            "Loading = 60\n",
            "Loading = 61\n",
            "Loading = 62\n",
            "Loading = 63\n",
            "Loading = 64\n",
            "Loading = 65\n",
            "Loading = 66\n",
            "Loading = 67\n",
            "Loading = 68\n",
            "Loading = 69\n",
            "Loading = 70\n",
            "Loading = 71\n",
            "Loading = 72\n",
            "Loading = 73\n",
            "Loading = 74\n",
            "Loading = 75\n",
            "Loading = 76\n",
            "Loading = 77\n",
            "Loading = 78\n",
            "Loading = 79\n",
            "Loading = 80\n",
            "Loading = 81\n",
            "Loading = 82\n",
            "Loading = 83\n",
            "Loading = 84\n",
            "Loading = 85\n",
            "Loading = 86\n",
            "Loading = 87\n",
            "Loading = 88\n",
            "Loading = 89\n",
            "Loading = 90\n",
            "Loading = 91\n",
            "Loading = 92\n",
            "Loading = 93\n",
            "Loading = 94\n",
            "Loading = 95\n",
            "Loading = 96\n",
            "Loading = 97\n",
            "Loading = 98\n",
            "Loading = 99\n",
            "Epoch = 1\n",
            "Epoch = 2\n",
            "Epoch = 3\n",
            "Epoch = 4\n",
            "Epoch = 5\n",
            "Epoch = 6\n",
            "Epoch = 7\n",
            "Epoch = 8\n",
            "Epoch = 9\n",
            "Epoch = 10\n",
            "Epoch = 11\n",
            "Epoch = 12\n",
            "Epoch = 13\n",
            "Epoch = 14\n",
            "Epoch = 15\n",
            "Epoch = 16\n",
            "Epoch = 17\n",
            "Epoch = 18\n",
            "Epoch = 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpbEO9LQsM03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_predict = rnn(torch.Tensor(test_x))\n",
        "test_predict = test_predict.detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TsJpAXPFEnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import gamma\n",
        "\n",
        "def meanW(ab):\n",
        "    return ab[0]*gamma(1+1/ab[1])\n",
        "def medianW(ab):\n",
        "    return ab[0]*(-np.log(.5))**(1/ab[1])\n",
        "def stdW(ab):\n",
        "    return ab[0]*np.sqrt(gamma(1+2/ab[1])-gamma(1+1/ab[1])**2)\n",
        "def weibull(x, ab):\n",
        "    a = ab[0]\n",
        "    b = ab[1]\n",
        "    y = np.copy(x)\n",
        "    for i in range(len(x)):\n",
        "        if x[i] > 0:\n",
        "            y[i] = b/a*((x[i]/a)**(b-1))*np.e**(-(x[i]/a)**b)\n",
        "        else:\n",
        "            y[i] = 0\n",
        "    return y\n",
        "\n",
        "case = 21\n",
        "x = np.linspace(0,test_predict[case][0]*2,1024)\n",
        "plt.axvline(test_y[case][0])\n",
        "y = weibull(x,test_predict[case])\n",
        "plt.plot(x,y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V0eWNXVd7uxO",
        "colab": {}
      },
      "source": [
        "life = np.zeros(len(test_y))\n",
        "for i in range(len(test_y)):\n",
        "    life[i] = test_y[i][0]\n",
        "basis = np.arange(len(life))+1\n",
        "order = np.argsort(life)\n",
        "\n",
        "predictions = np.zeros(len(test_predict))\n",
        "accuracy = np.zeros(len(test_predict))\n",
        "for i in range(len(test_predict)):\n",
        "    predictions[i] = medianW(test_predict[i])\n",
        "    accuracy[i] = stdW(test_predict[i])\n",
        "\n",
        "plt.scatter(basis,life[order])\n",
        "plt.errorbar(basis,predictions[order],accuracy[order])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnUHSANTHUx1",
        "colab_type": "text"
      },
      "source": [
        "What we are trying to do is to obtain a single value from a set of data. Implicitly is like if we censor the data using the failure event"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUg_gbwf2XeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
